{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch\n",
    "from transformers import BertTokenizer\n",
    "from torch.utils.data import Dataset,DataLoader\n",
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class my_dataset(Dataset):\n",
    "    def __init__(self,split):\n",
    "        self.dataset=load_dataset(\"seamew/Weibo\",split=split)\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "    def __getitem__(self,idx):\n",
    "        text=self.dataset[idx][\"text\"]\n",
    "        label=self.dataset[idx][\"label\"]\n",
    "        return text,label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset=my_dataset(\"train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_label=[]\n",
    "for i in range(len(train_dataset)):\n",
    "    if train_dataset[i][1] not in all_label:\n",
    "        all_label.append(train_dataset[i][1])\n",
    "len(all_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer\n",
    "token=BertTokenizer.from_pretrained(\"bert-base-chinese\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(data):\n",
    "    sents=[i[0] for i in data]\n",
    "    labels=[i[1] for i in data]\n",
    "    data = token.batch_encode_plus(\n",
    "        batch_text_or_text_pairs=sents,\n",
    "        truncation=True,\n",
    "        padding=\"max_length\",\n",
    "        max_length=32,\n",
    "        return_tensors='pt',\n",
    "        return_length=True)\n",
    "    input_ids = data['input_ids']\n",
    "    attention_mask = data['attention_mask']\n",
    "    token_type_ids = data['token_type_ids']\n",
    "    labels = torch.LongTensor(labels)\n",
    "    return input_ids, attention_mask, token_type_ids, labels\n",
    "loader=DataLoader(dataset=train_dataset,batch_size=16,shuffle=True,collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertModel\n",
    "pre_model=BertModel.from_pretrained(\"bert-base-chinese\")\n",
    "#for param in pre_model.parameters():\n",
    "#    param.requires_grad_(False)\n",
    "for i, (input_ids, attention_mask, token_type_ids,\n",
    "        labels) in enumerate(loader):\n",
    "    break\n",
    "out=pre_model(input_ids=input_ids,attention_mask=attention_mask,token_type_ids=token_type_ids)\n",
    "out.last_hidden_state.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#定义分类任务\n",
    "class Model(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.fc=nn.Linear(768,8)\n",
    "    def forward(self,input_ids, attention_mask, token_type_ids):\n",
    "        with torch.no_grad():\n",
    "            out = pre_model(input_ids=input_ids,\n",
    "                       attention_mask=attention_mask,\n",
    "                       token_type_ids=token_type_ids)\n",
    "            #只取第一个，也就是CLS进行预测\n",
    "        out=self.fc(out.last_hidden_state[:, 0])\n",
    "        out = out.softmax(dim=1)\n",
    "        return out\n",
    "model=Model()\n",
    "model(input_ids=input_ids,\n",
    "      attention_mask=attention_mask,\n",
    "      token_type_ids=token_type_ids).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AdamW\n",
    "optim=AdamW(model.parameters(),lr=1e-4)\n",
    "criterion=nn.CrossEntropyLoss()\n",
    "model.train()\n",
    "sched = torch.optim.lr_scheduler.StepLR(optim, step_size=1, gamma=0.5)\n",
    "for epoch in range(2):\n",
    "    for i,(input_ids, attention_mask, token_type_ids,labels) in enumerate(loader):\n",
    "        out=model(input_ids, attention_mask, token_type_ids)\n",
    "        loss=criterion(out,labels)\n",
    "        loss.backward()\n",
    "        optim.step()\n",
    "        optim.zero_grad()\n",
    "        if i % 5 == 0:\n",
    "            out = out.argmax(dim=1)\n",
    "            accuracy = (out == labels).sum().item() / len(labels)\n",
    "\n",
    "            print(i, loss.item(), accuracy)\n",
    "    sched.step()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "9265a1f83cbeb4cb496e9501fca05d914afba8201d048766b6d351b2b6609745"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
