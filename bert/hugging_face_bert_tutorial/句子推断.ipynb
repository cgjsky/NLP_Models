{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from datasets import load_dataset\n",
    "from torch.utils.data import DataLoader,Dataset\n",
    "from transformers import BertTokenizer,BertModel,AdamW\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class my_dataset(Dataset):\n",
    "    def __init__(self,split):\n",
    "        dataset=load_dataset(\"seamew/Weibo\",split=split)\n",
    "        def f(data):\n",
    "            return len(data[\"text\"])>20\n",
    "        self.dataset=dataset.filter(f)\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "    def __getitem__(self,i):\n",
    "        text=self.dataset[i][\"text\"]\n",
    "        sentence1 = text[:20]\n",
    "        sentence2 = text[20:40]\n",
    "        label = 0\n",
    "        if random.randint(0,1) == 0 :\n",
    "            j = random.randint(0, len(self.dataset) - 1)\n",
    "            sentence2 = self.dataset[j][\"text\"][20:40]\n",
    "            label = 1\n",
    "        return sentence1,sentence2,label\n",
    "train_data=my_dataset(\"train\")\n",
    "sentence1, sentence2, label = train_data[0]\n",
    "len(train_data),sentence1,sentence2,label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "token=BertTokenizer.from_pretrained(\"bert-base-chinese\")\n",
    "def collate_fn(data):\n",
    "    sents=[i[:2] for i in data]\n",
    "    labels=[i[2] for i in data]\n",
    "    data=token.batch_encode_plus(\n",
    "        batch_text_or_text_pairs=sents,\n",
    "        truncation=True,\n",
    "        padding=\"max_length\",\n",
    "        max_length=45,\n",
    "        return_tensors=\"pt\",\n",
    "        return_length=True,\n",
    "        add_special_tokens=True\n",
    "    )\n",
    "    attention_mask = data[\"attention_mask\"]\n",
    "    token_type_ids = data[\"token_type_ids\"]\n",
    "    input_ids = data[\"input_ids\"]\n",
    "    labels =torch.LongTensor(labels)\n",
    "    return input_ids,attention_mask,token_type_ids,labels\n",
    "loader=DataLoader(\n",
    "    dataset=train_data,\n",
    "    batch_size=64,\n",
    "    shuffle=True,\n",
    "    collate_fn=collate_fn,\n",
    "    drop_last=True\n",
    "    )\n",
    "for i,(input_ids,attention_mask,token_type_ids,labels) in enumerate(loader):\n",
    "    break\n",
    "print(len(loader))\n",
    "print(token.decode(input_ids[0]))\n",
    "input_ids.shape,attention_mask.shape,token_type_ids.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pre_model=BertModel.from_pretrained(\"bert-base-chinese\")\n",
    "for param in pre_model.parameters():\n",
    "    param.requires_grad_(False)\n",
    "out=pre_model(input_ids,attention_mask,token_type_ids)\n",
    "out.last_hidden_state.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#下游任务\n",
    "class Model(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.fc=nn.Linear(768,2)\n",
    "    def forward(self,input_ids,attention_mask,token_type_ids):\n",
    "        with torch.no_grad():\n",
    "            out = pre_model(input_ids=input_ids,\n",
    "                             attention_mask=attention_mask,\n",
    "                             token_type_ids=token_type_ids)\n",
    "        out =self.fc(out.last_hidden_state[:,0])\n",
    "        out =out.softmax(dim=1)\n",
    "        return out\n",
    "model=Model()\n",
    "model(input_ids=input_ids,\n",
    "      attention_mask=attention_mask,\n",
    "      token_type_ids=token_type_ids).shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optim=AdamW(model.parameters(),lr=5e-4)\n",
    "criteration=nn.CrossEntropyLoss()\n",
    "model.train()\n",
    "for i,(input_ids,attention_mask,token_type_ids,labels) in enumerate(loader):\n",
    "    out = model(input_ids,attention_mask,token_type_ids)\n",
    "    loss = criteration(out,labels)\n",
    "    loss.backward()\n",
    "    optim.step()\n",
    "    optim.zero_grad()\n",
    "    if i % 5 == 0:\n",
    "        out = out.argmax(dim=1)\n",
    "        accuracy = (out == labels).sum().item() / len(labels)\n",
    "        print(i, loss.item(), accuracy)\n",
    "\n",
    "    if i == 300:\n",
    "        break\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "9265a1f83cbeb4cb496e9501fca05d914afba8201d048766b6d351b2b6609745"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
